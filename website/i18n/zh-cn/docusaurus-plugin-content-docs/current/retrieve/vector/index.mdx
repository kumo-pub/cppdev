---
title: Vector Search Library Selection Guide (Production-Ops Focused)
sidebar_label: Library Selection
---

## 1. Overview of Mainstream Open-Source Vector Search Libraries
This section lists production-proven C++ vector search libraries suitable for practical deployment, excluding academic-only, Python-only, or non-C++ libraries.

| Library Name | Core Developer | Core Strengths | Operational Complexity | Maintenance Status |
|--------------|---------------|----------------|------------------------|--------------------|
| FAISS | Meta AI | Industrial standard, CPU/GPU hybrid support, multiple algorithms (PQ/IVF/HNSW), large-scale support | Medium (GPU tuning needed) | High (active updates, rich docs) |
| DiskANN | Microsoft | SSD-optimized, ultra-low RAM usage, disk-based large-scale storage | Medium (disk I/O tuning required) | Medium (stable updates, enterprise cases) |
| NMSLIB | Yandex | Stable CPU-only performance, multi-algorithm support | Low (plug-and-play) | Medium (mature, moderate updates) |
| NGT | Yahoo Japan | Native dynamic CRUD for small-scale datasets, high recall for high-dimensional data | Low (single-node only) | Medium (stable for dynamic scenarios) |
| HNSWLIB | C++ | HNSW, incremental add & delete | Low | High (active, production-proven) |

> Removed non-C++ or non-production libraries: Spotify Annoy, Weaviate, SPTAG, SCANN, Vespa.ai, RAFT.

---

## 2. Scenario-Based Selection Recommendations (With Production Context)
### 2.1 Lightweight Deployment (Edge / Small-Scale Datasets ≤5M Vectors)
**Core Requirements**: Zero-dependency integration, minimal footprint, real-time dynamic updates
- **Primary Recommendation**: HNSWLIB
- Production Fit: Single-node, supports incremental addition and deletion, ideal for ≤5M vectors.
- Ops Note: Batch updates recommended to reduce index fragmentation; best for dynamic CRUD.
- **Alternative**: NGT
- Production Fit: Supports partial incremental insertions, deletion limited; single-node.
- Ops Note: Not ideal for high-frequency updates; still requires batch writes for stability.

### 2.2 Medium-Scale On-Prem Deployment (5M~100M Vectors / CPU-Only)
**Core Requirements**: Stable single-node performance, memory-efficient storage
- **Primary Recommendation**: FAISS (CPU)
- Production Fit: `IndexIVFPQ` for memory-efficient storage; suitable for up to ~100M vectors at 768-dim.
- Ops Note: Incremental additions possible for very small batches; deletion requires rebuild or soft delete strategy.
- **Alternative**: NMSLIB
- Production Fit: CPU-only HNSW; reliable and easy to maintain for medium-scale datasets.
- Ops Note: No native incremental deletion; static index preferred for consistent performance.

### 2.3 Large-Scale / SSD-Optimized Datasets (>100M Vectors)
**Core Requirements**: Disk-backed storage, ultra-low RAM, static or batch-updated indices
- **Primary Recommendation**: DiskANN
- Production Fit: Supports 100M–1B+ vectors with minimal RAM; HNSW on SSD for high recall.
- Ops Note: Batch-only updates; not suitable for real-time CRUD.
- **Alternative**: FAISS (Distributed CPU)
- Production Fit: Shard data across nodes; use `IndexIVFPQ` or IVF-PQ for large-scale deployment.
- Ops Note: Requires shared storage for index synchronization; higher ops overhead than DiskANN.

### 2.4 GPU-Accelerated Low-Latency Retrieval
**Core Requirements**: Sub-millisecond query latency, high throughput
- **Primary Recommendation**: FAISS (GPU)
- Production Fit: `GpuIndexHNSW` `<0.5ms` latency for ~100M vectors (768-dim); single GPU can handle 10k+ QPS.
- Ops Note: Monitor VRAM to prevent OOM; suitable for read-heavy workloads.

---

## 3. Real-Time CRUD Capability (Dynamic Index Updates)
| Library | Incremental Add | Incremental Delete | Notes |
|---------|----------------|-----------------|-------|
| HNSWLIB | Yes | Yes | Single-node; best for datasets ≤5M; batch writes recommended. |
| NGT | Partial | Limited | Supports incremental insertions; deletion restricted; single-node only. |
| FAISS | Limited | No | Only very small datasets (~0.5M) support incremental add; deletion requires rebuild. |
| DiskANN | No | No | Designed for static, large-scale, disk-based indices; real-time CRUD not supported. |

---

## 4. Key Selection Principles for Production
1. **Prioritize C++ Ecosystem**: Keep integration simple, single-node deployment preferred for small teams.
2. **Match Real-Time Needs**: Only HNSWLIB and NGT (limited) support dynamic CRUD; FAISS incremental add feasible for tiny datasets.
3. **Scale Appropriately**: ≤5M vectors → HNSWLIB; 5M–100M → FAISS CPU; >100M → DiskANN or distributed FAISS.
4. **GPU Acceleration**: Only for high-throughput, low-latency read-heavy scenarios.
5. **Distributed Considerations**: Focus on single-node; distributed systems are combinations of single-node instances.


## 5. Production Considerations

In practice, most production vector search systems **do not rely solely on open-source libraries**. Instead, they are usually **adapted or extended from existing algorithms** to meet the requirements of **online real-time indexing, dynamic CRUD, and operational stability**. For example, **Kumo builds upon HNSW**, adding enhancements to fully support **online real-time indexing** while maintaining low operational overhead.

That said, if production requirements are not extremely stringent, **selecting an open-source library is entirely reasonable**.

### GPU Usage
While GPUs theoretically accelerate vector computations, actual performance can be **limited by memory bandwidth and CPU-GPU transfer speeds**. Benchmarking under realistic workloads is critical before committing to GPU deployment.

### Multi-threading / OpenMP
Multi-threading can improve throughput, but **thread-switching overhead may negate speedups** beyond a certain threshold. **OpenMP benchmarks** should be performed to identify the optimal parallelism level.

### SIMD Usage
SIMD instructions can increase computation speed but may **reduce numerical precision**, causing slight errors in distance or aggregation calculations. In **e-commerce scenarios**, even small precision loss can impact ranking or pricing computations. Validate precision impacts carefully before enabling aggressive SIMD optimizations.
