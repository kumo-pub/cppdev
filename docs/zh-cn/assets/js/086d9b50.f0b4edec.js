"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[895],{8372:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"testing/benchmark/tools","title":"Benchmark Tools","description":"compare.py","source":"@site/docs/testing/benchmark/tools.md","sourceDirName":"testing/benchmark","slug":"/testing/benchmark/tools","permalink":"/cppdev/zh-cn/docs/testing/benchmark/tools","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/testing/benchmark/tools.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"How to release","permalink":"/cppdev/zh-cn/docs/testing/benchmark/releasing"},"next":{"title":"User Guide","permalink":"/cppdev/zh-cn/docs/testing/benchmark/user_guide"}}');var r=s(4848),i=s(8453);const a={},o="Benchmark Tools",c={},h=[{value:"compare.py",id:"comparepy",level:2},{value:"Dependencies",id:"dependencies",level:3},{value:"Displaying aggregates only",id:"displaying-aggregates-only",level:3},{value:"Modes of operation",id:"modes-of-operation",level:3},{value:"Note: Interpreting the output",id:"note-interpreting-the-output",level:3},{value:"U test",id:"u-test",level:3}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"benchmark-tools",children:"Benchmark Tools"})}),"\n",(0,r.jsx)(n.h2,{id:"comparepy",children:"compare.py"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"compare.py"})," can be used to compare the result of benchmarks."]}),"\n",(0,r.jsx)(n.h3,{id:"dependencies",children:"Dependencies"}),"\n",(0,r.jsxs)(n.p,{children:["The utility relies on the ",(0,r.jsx)(n.a,{href:"https://www.scipy.org",children:"scipy"})," package which can be installed using pip:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install -r requirements.txt\n"})}),"\n",(0,r.jsx)(n.h3,{id:"displaying-aggregates-only",children:"Displaying aggregates only"}),"\n",(0,r.jsxs)(n.p,{children:["The switch ",(0,r.jsx)(n.code,{children:"-a"})," / ",(0,r.jsx)(n.code,{children:"--display_aggregates_only"})," can be used to control the\ndisplayment of the normal iterations vs the aggregates. When passed, it will\nbe passthrough to the benchmark binaries to be run, and will be accounted for\nin the tool itself; only the aggregates will be displayed, but not normal runs.\nIt only affects the display, the separate runs will still be used to calculate\nthe U test."]}),"\n",(0,r.jsx)(n.h3,{id:"modes-of-operation",children:"Modes of operation"}),"\n",(0,r.jsx)(n.p,{children:"There are three modes of operation:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Just compare two benchmarks\nThe program is invoked like:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ compare.py benchmarks <benchmark_baseline> <benchmark_contender> [benchmark options]...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"<benchmark_baseline>"})," and ",(0,r.jsx)(n.code,{children:"<benchmark_contender>"})," either specify a benchmark executable file, or a JSON output file. The type of the input file is automatically detected. If a benchmark executable is specified then the benchmark is run to obtain the results. Otherwise the results are simply loaded from the output file."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"[benchmark options]"})," will be passed to the benchmarks invocations. They can be anything that binary accepts, be it either normal ",(0,r.jsx)(n.code,{children:"--benchmark_*"})," parameters, or some custom parameters your binary takes."]}),"\n",(0,r.jsx)(n.p,{children:"Example output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$ ./compare.py benchmarks ./a.out ./a.out\nRUNNING: ./a.out --benchmark_out=/tmp/tmprBT5nW\nRun on (8 X 4000 MHz CPU s)\n2017-11-07 21:16:44\n------------------------------------------------------\nBenchmark               Time           CPU Iterations\n------------------------------------------------------\nBM_memcpy/8            36 ns         36 ns   19101577   211.669MB/s\nBM_memcpy/64           76 ns         76 ns    9412571   800.199MB/s\nBM_memcpy/512          84 ns         84 ns    8249070   5.64771GB/s\nBM_memcpy/1024        116 ns        116 ns    6181763   8.19505GB/s\nBM_memcpy/8192        643 ns        643 ns    1062855   11.8636GB/s\nBM_copy/8             222 ns        222 ns    3137987   34.3772MB/s\nBM_copy/64           1608 ns       1608 ns     432758   37.9501MB/s\nBM_copy/512         12589 ns      12589 ns      54806   38.7867MB/s\nBM_copy/1024        25169 ns      25169 ns      27713   38.8003MB/s\nBM_copy/8192       201165 ns     201112 ns       3486   38.8466MB/s\nRUNNING: ./a.out --benchmark_out=/tmp/tmpt1wwG_\nRun on (8 X 4000 MHz CPU s)\n2017-11-07 21:16:53\n------------------------------------------------------\nBenchmark               Time           CPU Iterations\n------------------------------------------------------\nBM_memcpy/8            36 ns         36 ns   19397903   211.255MB/s\nBM_memcpy/64           73 ns         73 ns    9691174   839.635MB/s\nBM_memcpy/512          85 ns         85 ns    8312329   5.60101GB/s\nBM_memcpy/1024        118 ns        118 ns    6438774   8.11608GB/s\nBM_memcpy/8192        656 ns        656 ns    1068644   11.6277GB/s\nBM_copy/8             223 ns        223 ns    3146977   34.2338MB/s\nBM_copy/64           1611 ns       1611 ns     435340   37.8751MB/s\nBM_copy/512         12622 ns      12622 ns      54818   38.6844MB/s\nBM_copy/1024        25257 ns      25239 ns      27779   38.6927MB/s\nBM_copy/8192       205013 ns     205010 ns       3479    38.108MB/s\nComparing ./a.out to ./a.out\nBenchmark                 Time             CPU      Time Old      Time New       CPU Old       CPU New\n------------------------------------------------------------------------------------------------------\nBM_memcpy/8            +0.0020         +0.0020            36            36            36            36\nBM_memcpy/64           -0.0468         -0.0470            76            73            76            73\nBM_memcpy/512          +0.0081         +0.0083            84            85            84            85\nBM_memcpy/1024         +0.0098         +0.0097           116           118           116           118\nBM_memcpy/8192         +0.0200         +0.0203           643           656           643           656\nBM_copy/8              +0.0046         +0.0042           222           223           222           223\nBM_copy/64             +0.0020         +0.0020          1608          1611          1608          1611\nBM_copy/512            +0.0027         +0.0026         12589         12622         12589         12622\nBM_copy/1024           +0.0035         +0.0028         25169         25257         25169         25239\nBM_copy/8192           +0.0191         +0.0194        201165        205013        201112        205010\n"})}),"\n",(0,r.jsxs)(n.p,{children:["What it does is for the every benchmark from the first run it looks for the benchmark with exactly the same name in the second run, and then compares the results. If the names differ, the benchmark is omitted from the diff.\nAs you can note, the values in ",(0,r.jsx)(n.code,{children:"Time"})," and ",(0,r.jsx)(n.code,{children:"CPU"})," columns are calculated as ",(0,r.jsx)(n.code,{children:"(new - old) / |old|"}),"."]}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Compare two different filters of one benchmark\nThe program is invoked like:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ compare.py filters <benchmark> <filter_baseline> <filter_contender> [benchmark options]...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"<benchmark>"})," either specify a benchmark executable file, or a JSON output file. The type of the input file is automatically detected. If a benchmark executable is specified then the benchmark is run to obtain the results. Otherwise the results are simply loaded from the output file."]}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"<filter_baseline>"})," and ",(0,r.jsx)(n.code,{children:"<filter_contender>"})," are the same regex filters that you would pass to the ",(0,r.jsx)(n.code,{children:"[--benchmark_filter=<regex>]"})," parameter of the benchmark binary."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"[benchmark options]"})," will be passed to the benchmarks invocations. They can be anything that binary accepts, be it either normal ",(0,r.jsx)(n.code,{children:"--benchmark_*"})," parameters, or some custom parameters your binary takes."]}),"\n",(0,r.jsx)(n.p,{children:"Example output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$ ./compare.py filters ./a.out BM_memcpy BM_copy\nRUNNING: ./a.out --benchmark_filter=BM_memcpy --benchmark_out=/tmp/tmpBWKk0k\nRun on (8 X 4000 MHz CPU s)\n2017-11-07 21:37:28\n------------------------------------------------------\nBenchmark               Time           CPU Iterations\n------------------------------------------------------\nBM_memcpy/8            36 ns         36 ns   17891491   211.215MB/s\nBM_memcpy/64           74 ns         74 ns    9400999   825.646MB/s\nBM_memcpy/512          87 ns         87 ns    8027453   5.46126GB/s\nBM_memcpy/1024        111 ns        111 ns    6116853    8.5648GB/s\nBM_memcpy/8192        657 ns        656 ns    1064679   11.6247GB/s\nRUNNING: ./a.out --benchmark_filter=BM_copy --benchmark_out=/tmp/tmpAvWcOM\nRun on (8 X 4000 MHz CPU s)\n2017-11-07 21:37:33\n----------------------------------------------------\nBenchmark             Time           CPU Iterations\n----------------------------------------------------\nBM_copy/8           227 ns        227 ns    3038700   33.6264MB/s\nBM_copy/64         1640 ns       1640 ns     426893   37.2154MB/s\nBM_copy/512       12804 ns      12801 ns      55417   38.1444MB/s\nBM_copy/1024      25409 ns      25407 ns      27516   38.4365MB/s\nBM_copy/8192     202986 ns     202990 ns       3454   38.4871MB/s\nComparing BM_memcpy to BM_copy (from ./a.out)\nBenchmark                               Time             CPU      Time Old      Time New       CPU Old       CPU New\n--------------------------------------------------------------------------------------------------------------------\n[BM_memcpy vs. BM_copy]/8            +5.2829         +5.2812            36           227            36           227\n[BM_memcpy vs. BM_copy]/64          +21.1719        +21.1856            74          1640            74          1640\n[BM_memcpy vs. BM_copy]/512        +145.6487       +145.6097            87         12804            87         12801\n[BM_memcpy vs. BM_copy]/1024       +227.1860       +227.1776           111         25409           111         25407\n[BM_memcpy vs. BM_copy]/8192       +308.1664       +308.2898           657        202986           656        202990\n"})}),"\n",(0,r.jsxs)(n.p,{children:["As you can see, it applies filter to the benchmarks, both when running the benchmark, and before doing the diff. And to make the diff work, the matches are replaced with some common string. Thus, you can compare two different benchmark families within one benchmark binary.\nAs you can note, the values in ",(0,r.jsx)(n.code,{children:"Time"})," and ",(0,r.jsx)(n.code,{children:"CPU"})," columns are calculated as ",(0,r.jsx)(n.code,{children:"(new - old) / |old|"}),"."]}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Compare filter one from benchmark one to filter two from benchmark two:\nThe program is invoked like:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ compare.py filters <benchmark_baseline> <filter_baseline> <benchmark_contender> <filter_contender> [benchmark options]...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"<benchmark_baseline>"})," and ",(0,r.jsx)(n.code,{children:"<benchmark_contender>"})," either specify a benchmark executable file, or a JSON output file. The type of the input file is automatically detected. If a benchmark executable is specified then the benchmark is run to obtain the results. Otherwise the results are simply loaded from the output file."]}),"\n",(0,r.jsxs)(n.p,{children:["Where ",(0,r.jsx)(n.code,{children:"<filter_baseline>"})," and ",(0,r.jsx)(n.code,{children:"<filter_contender>"})," are the same regex filters that you would pass to the ",(0,r.jsx)(n.code,{children:"[--benchmark_filter=<regex>]"})," parameter of the benchmark binary."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"[benchmark options]"})," will be passed to the benchmarks invocations. They can be anything that binary accepts, be it either normal ",(0,r.jsx)(n.code,{children:"--benchmark_*"})," parameters, or some custom parameters your binary takes."]}),"\n",(0,r.jsx)(n.p,{children:"Example output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$ ./compare.py benchmarksfiltered ./a.out BM_memcpy ./a.out BM_copy\nRUNNING: ./a.out --benchmark_filter=BM_memcpy --benchmark_out=/tmp/tmp_FvbYg\nRun on (8 X 4000 MHz CPU s)\n2017-11-07 21:38:27\n------------------------------------------------------\nBenchmark               Time           CPU Iterations\n------------------------------------------------------\nBM_memcpy/8            37 ns         37 ns   18953482   204.118MB/s\nBM_memcpy/64           74 ns         74 ns    9206578   828.245MB/s\nBM_memcpy/512          91 ns         91 ns    8086195   5.25476GB/s\nBM_memcpy/1024        120 ns        120 ns    5804513   7.95662GB/s\nBM_memcpy/8192        664 ns        664 ns    1028363   11.4948GB/s\nRUNNING: ./a.out --benchmark_filter=BM_copy --benchmark_out=/tmp/tmpDfL5iE\nRun on (8 X 4000 MHz CPU s)\n2017-11-07 21:38:32\n----------------------------------------------------\nBenchmark             Time           CPU Iterations\n----------------------------------------------------\nBM_copy/8           230 ns        230 ns    2985909   33.1161MB/s\nBM_copy/64         1654 ns       1653 ns     419408   36.9137MB/s\nBM_copy/512       13122 ns      13120 ns      53403   37.2156MB/s\nBM_copy/1024      26679 ns      26666 ns      26575   36.6218MB/s\nBM_copy/8192     215068 ns     215053 ns       3221   36.3283MB/s\nComparing BM_memcpy (from ./a.out) to BM_copy (from ./a.out)\nBenchmark                               Time             CPU      Time Old      Time New       CPU Old       CPU New\n--------------------------------------------------------------------------------------------------------------------\n[BM_memcpy vs. BM_copy]/8            +5.1649         +5.1637            37           230            37           230\n[BM_memcpy vs. BM_copy]/64          +21.4352        +21.4374            74          1654            74          1653\n[BM_memcpy vs. BM_copy]/512        +143.6022       +143.5865            91         13122            91         13120\n[BM_memcpy vs. BM_copy]/1024       +221.5903       +221.4790           120         26679           120         26666\n[BM_memcpy vs. BM_copy]/8192       +322.9059       +323.0096           664        215068           664        215053\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This is a mix of the previous two modes, two (potentially different) benchmark binaries are run, and a different filter is applied to each one.\nAs you can note, the values in ",(0,r.jsx)(n.code,{children:"Time"})," and ",(0,r.jsx)(n.code,{children:"CPU"})," columns are calculated as ",(0,r.jsx)(n.code,{children:"(new - old) / |old|"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"note-interpreting-the-output",children:"Note: Interpreting the output"}),"\n",(0,r.jsx)(n.p,{children:"Performance measurements are an art, and performance comparisons are doubly so.\nResults are often noisy and don't necessarily have large absolute differences to\nthem, so just by visual inspection, it is not at all apparent if two\nmeasurements are actually showing a performance change or not. It is even more\nconfusing with multiple benchmark repetitions."}),"\n",(0,r.jsxs)(n.p,{children:["Thankfully, what we can do, is use statistical tests on the results to determine\nwhether the performance has statistically-significantly changed. ",(0,r.jsx)(n.code,{children:"compare.py"}),"\nuses ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test",children:"Mann\u2013Whitney U\ntest"}),", with a null\nhypothesis being that there's no difference in performance."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"The below output is a summary of a benchmark comparison with statistics\nprovided for a multi-threaded process."})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Benchmark                                               Time        CPU    Time Old      Time New       CPU Old       CPU New\n-----------------------------------------------------------------------------------------------------------------------------\nbenchmark/threads:1/process_time/real_time_pvalue     0.0000     0.0000    U Test, Repetitions: 27 vs 27\nbenchmark/threads:1/process_time/real_time_mean      -0.1442    -0.1442          90            77            90            77\nbenchmark/threads:1/process_time/real_time_median    -0.1444    -0.1444          90            77            90            77\nbenchmark/threads:1/process_time/real_time_stddev    +0.3974    +0.3933           0             0             0             0\nbenchmark/threads:1/process_time/real_time_cv        +0.6329    +0.6280           0             0             0             0\nOVERALL_GEOMEAN                                      -0.1442    -0.1442           0             0             0             0\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"Here's a breakdown of each row:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"benchmark/threads:1/process_time/real_time_pvalue"}),": This shows the ",(0,r.jsx)(n.em,{children:"p-value"})," for\nthe statistical test comparing the performance of the process running with one\nthread. A value of 0.0000 suggests a statistically significant difference in\nperformance. The comparison was conducted using the U Test (Mann-Whitney\nU Test) with 27 repetitions for each case."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"benchmark/threads:1/process_time/real_time_mean"}),": This shows the relative\ndifference in mean execution time between two different cases. The negative\nvalue (-0.1442) implies that the new process is faster by about 14.42%. The old\ntime was 90 units, while the new time is 77 units."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"benchmark/threads:1/process_time/real_time_median"}),": Similarly, this shows the\nrelative difference in the median execution time. Again, the new process is\nfaster by 14.44%."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"benchmark/threads:1/process_time/real_time_stddev"}),": This is the relative\ndifference in the standard deviation of the execution time, which is a measure\nof how much variation or dispersion there is from the mean. A positive value\n(+0.3974) implies there is more variance in the execution time in the new\nprocess."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"benchmark/threads:1/process_time/real_time_cv"}),": CV stands for Coefficient of\nVariation. It is the ratio of the standard deviation to the mean. It provides a\nstandardized measure of dispersion. An increase (+0.6329) indicates more\nrelative variability in the new process."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"OVERALL_GEOMEAN"}),": Geomean stands for geometric mean, a type of average that is\nless influenced by outliers. The negative value indicates a general improvement\nin the new process. However, given the values are all zero for the old and new\ntimes, this seems to be a mistake or placeholder in the output."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:["Let's first try to see what the different columns represent in the above\n",(0,r.jsx)(n.code,{children:"compare.py"})," benchmarking output:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benchmark:"})," The name of the function being benchmarked, along with the\nsize of the input (after the slash)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time:"})," The average time per operation, across all iterations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"CPU:"})," The average CPU time per operation, across all iterations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Iterations:"})," The number of iterations the benchmark was run to get a\nstable estimate."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Old and Time New:"})," These represent the average time it takes for a\nfunction to run in two different scenarios or versions. For example, you\nmight be comparing how fast a function runs before and after you make some\nchanges to it."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"CPU Old and CPU New:"})," These show the average amount of CPU time that the\nfunction uses in two different scenarios or versions. This is similar to\nTime Old and Time New, but focuses on CPU usage instead of overall time."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In the comparison section, the relative differences in both time and CPU time\nare displayed for each input size."}),"\n",(0,r.jsxs)(n.p,{children:["A statistically-significant difference is determined by a ",(0,r.jsx)(n.strong,{children:"p-value"}),", which is\na measure of the probability that the observed difference could have occurred\njust by random chance. A smaller p-value indicates stronger evidence against the\nnull hypothesis."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Therefore:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"If the p-value is less than the chosen significance level (alpha), we\nreject the null hypothesis and conclude the benchmarks are significantly\ndifferent."}),"\n",(0,r.jsx)(n.li,{children:"If the p-value is greater than or equal to alpha, we fail to reject the\nnull hypothesis and treat the two benchmarks as similar."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The result of said the statistical test is additionally communicated through color coding:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-diff",children:"+ Green:\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The benchmarks are ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"statistically different"})}),". This could mean the\nperformance has either ",(0,r.jsx)(n.strong,{children:"significantly improved"})," or ",(0,r.jsx)(n.strong,{children:"significantly\ndeteriorated"}),". You should look at the actual performance numbers to see which\nis the case."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-diff",children:"- Red:\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The benchmarks are ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.strong,{children:"statistically similar"})}),". This means the performance\n",(0,r.jsx)(n.strong,{children:"hasn't significantly changed"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["In statistical terms, ",(0,r.jsx)(n.strong,{children:"'green'"})," means we reject the null hypothesis that\nthere's no difference in performance, and ",(0,r.jsx)(n.strong,{children:"'red'"})," means we fail to reject the\nnull hypothesis. This might seem counter-intuitive if you're expecting 'green'\nto mean 'improved performance' and 'red' to mean 'worsened performance'."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"  But remember, in this context:\n\n    'Success' means 'successfully finding a difference'.\n    'Failure' means 'failing to find a difference'.\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Also, please note that ",(0,r.jsx)(n.strong,{children:"even if"})," we determine that there ",(0,r.jsx)(n.strong,{children:"is"})," a\nstatistically-significant difference between the two measurements, it does not\n",(0,r.jsx)(n.em,{children:"necessarily"})," mean that the actual benchmarks that were measured ",(0,r.jsx)(n.strong,{children:"are"}),"\ndifferent, or vice versa, even if we determine that there is ",(0,r.jsx)(n.strong,{children:"no"}),"\nstatistically-significant difference between the two measurements, it does not\nnecessarily mean that the actual benchmarks that were measured ",(0,r.jsx)(n.strong,{children:"are not"}),"\ndifferent."]}),"\n",(0,r.jsx)(n.h3,{id:"u-test",children:"U test"}),"\n",(0,r.jsxs)(n.p,{children:["If there is a sufficient repetition count of the benchmarks, the tool can do\na ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test",children:"U Test"}),", of the\nnull hypothesis that it is equally likely that a randomly selected value from\none sample will be less than or greater than a randomly selected value from a\nsecond sample."]}),"\n",(0,r.jsx)(n.p,{children:"If the calculated p-value is below this value is lower than the significance\nlevel alpha, then the result is said to be statistically significant and the\nnull hypothesis is rejected. Which in other words means that the two benchmarks\naren't identical."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"WARNING"}),": requires ",(0,r.jsx)(n.strong,{children:"LARGE"})," (no less than 9) number of repetitions to be\nmeaningful!"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(6540);const r={},i=t.createContext(r);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);