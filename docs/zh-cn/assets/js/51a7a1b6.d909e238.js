"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8091],{8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var r=i(6540);const t={},s=r.createContext(t);function l(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(s.Provider,{value:n},e.children)}},8463:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"retrieve/nlp/sentencepiece","title":"SentencePiece \u2013 Subword Tokenization","description":"- Project: SentencePiece GitHub","source":"@site/i18n/zh-cn/docusaurus-plugin-content-docs/current/retrieve/nlp/sentencepiece.mdx","sourceDirName":"retrieve/nlp","slug":"/retrieve/nlp/sentencepiece","permalink":"/cppdev/zh-cn/docs/retrieve/nlp/sentencepiece","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Jieba \u2013 Chinese Word Segmentation","permalink":"/cppdev/zh-cn/docs/retrieve/nlp/jieba"},"next":{"title":"libstemmer \u2013 Snowball Stemming Library","permalink":"/cppdev/zh-cn/docs/retrieve/nlp/stemming"}}');var t=i(4848),s=i(8453);const l={},o="SentencePiece \u2013 Subword Tokenization",c={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Typical Use Cases",id:"typical-use-cases",level:2},{value:"Industrial Fit",id:"industrial-fit",level:2}];function a(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"sentencepiece--subword-tokenization",children:"SentencePiece \u2013 Subword Tokenization"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Project:"})," ",(0,t.jsx)(n.a,{href:"https://github.com/google/sentencepiece",children:"SentencePiece GitHub"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language:"})," C++"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["SentencePiece is a ",(0,t.jsx)(n.strong,{children:"language-independent subword tokenizer"})," designed for ",(0,t.jsx)(n.strong,{children:"NLP preprocessing"}),", especially for neural machine translation and text modeling. It converts raw text into ",(0,t.jsx)(n.strong,{children:"subword units"})," using statistical algorithms such as ",(0,t.jsx)(n.strong,{children:"BPE (Byte-Pair Encoding)"})," or ",(0,t.jsx)(n.strong,{children:"Unigram Language Model"}),", enabling robust handling of ",(0,t.jsx)(n.strong,{children:"OOV (out-of-vocabulary) words"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The library treats input text as a ",(0,t.jsx)(n.strong,{children:"sequence of Unicode characters"}),", making it fully compatible with ",(0,t.jsx)(n.strong,{children:"any language"}),", including Chinese, Japanese, and multilingual corpora. It is optimized for ",(0,t.jsx)(n.strong,{children:"offline preprocessing and large-scale tokenization pipelines"}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language-independent subword tokenization"})," for robust NLP preprocessing."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"BPE and Unigram LM algorithms"})," for subword segmentation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model training and inference"}),": Train your own subword model or use pre-trained models."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unicode support"}),": Fully compatible with UTF-8 text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deterministic tokenization"}),": Same input always produces the same token sequence."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"C++ API with Python bindings"}),": Easy integration in pipelines."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"typical-use-cases",children:"Typical Use Cases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Preprocessing text for ",(0,t.jsx)(n.strong,{children:"neural machine translation (NMT)"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Tokenization for ",(0,t.jsx)(n.strong,{children:"language modeling"})," and subword embedding."]}),"\n",(0,t.jsxs)(n.li,{children:["Handling rare words and multilingual corpora in ",(0,t.jsx)(n.strong,{children:"search engines"})," or NLP pipelines."]}),"\n",(0,t.jsxs)(n.li,{children:["Offline or batch ",(0,t.jsx)(n.strong,{children:"text normalization and segmentation"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"industrial-fit",children:"Industrial Fit"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Requirement"}),(0,t.jsx)(n.th,{children:"SentencePiece Support"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Subword tokenization"}),(0,t.jsx)(n.td,{children:"\u2714\ufe0f"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"OOV word handling"}),(0,t.jsx)(n.td,{children:"\u2714\ufe0f"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Unicode / multilingual support"}),(0,t.jsx)(n.td,{children:"\u2714\ufe0f"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"High-volume preprocessing"}),(0,t.jsx)(n.td,{children:"\u2714\ufe0f"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Model training / inference"}),(0,t.jsx)(n.td,{children:"\u2714\ufe0f"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}}}]);