"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[6811],{461:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"foundamentals/fibers/index","title":"Fiber Pattern Selection: A Multi-threaded Contention Perspective (Supplemented with Memory Contention Management Costs)","description":"1. Selection Background","source":"@site/docs/foundamentals/fibers/index.md","sourceDirName":"foundamentals/fibers","slug":"/foundamentals/fibers/","permalink":"/cppdev/zh-cn/docs/foundamentals/fibers/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/foundamentals/fibers/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Usage Summary of melon compression Module","permalink":"/cppdev/zh-cn/docs/foundamentals/compress/melon"},"next":{"title":"Futures","permalink":"/cppdev/zh-cn/docs/foundamentals/fibers/futures"}}');var t=i(4848),r=i(8453);const s={},c="Fiber Pattern Selection: A Multi-threaded Contention Perspective (Supplemented with Memory Contention Management Costs)",a={},l=[{value:"1. Selection Background",id:"1-selection-background",level:2},{value:"2. Core Architecture and Characteristics of Each Model (Supplemented with Memory Contention Management)",id:"2-core-architecture-and-characteristics-of-each-model-supplemented-with-memory-contention-management",level:2},{value:"2.1 N:1 Coroutines (Single-threaded)",id:"21-n1-coroutines-single-threaded",level:3},{value:"2.2 M kthread",id:"22-m-kthread",level:3},{value:"2.3 melon Fiber",id:"23-melon-fiber",level:3},{value:"2.4 ExecutionQueue",id:"24-executionqueue",level:3},{value:"2.5 Mutex+pthread",id:"25-mutexpthread",level:3},{value:"3. Comprehensive Comparison of All Models (Added Memory Contention Management Cost Dimension)",id:"3-comprehensive-comparison-of-all-models-added-memory-contention-management-cost-dimension",level:2},{value:"4. Precise Selection Recommendations (Incorporating Memory Contention Management Costs)",id:"4-precise-selection-recommendations-incorporating-memory-contention-management-costs",level:2},{value:"4.1 Priority Selection: M kthread",id:"41-priority-selection-m-kthread",level:3},{value:"4.2 Priority Selection: melon Fiber",id:"42-priority-selection-melon-fiber",level:3},{value:"4.3 Priority Selection: ExecutionQueue",id:"43-priority-selection-executionqueue",level:3},{value:"4.4 Priority Selection: N:1 Coroutines",id:"44-priority-selection-n1-coroutines",level:3},{value:"5. Conclusion",id:"5-conclusion",level:2},{value:"6. Final Summary",id:"6-final-summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"fiber-pattern-selection-a-multi-threaded-contention-perspective-supplemented-with-memory-contention-management-costs",children:"Fiber Pattern Selection: A Multi-threaded Contention Perspective (Supplemented with Memory Contention Management Costs)"})}),"\n",(0,t.jsx)(n.h2,{id:"1-selection-background",children:"1. Selection Background"}),"\n",(0,t.jsxs)(n.p,{children:["In high-concurrency service development, ",(0,t.jsx)(n.strong,{children:"multi-threaded contention"})," (including CPU contention and memory contention) is a core pain point restricting system performance and stability. Traditional kernel-level threads (pthread) rely on native locking mechanisms, which are prone to lock contention, deadlocks, and cache thrashing in high-contention scenarios. Meanwhile, the management costs of memory contention (shared memory synchronization, stack resource management, and cache coherence maintenance) are often overlooked, yet they directly determine development complexity and runtime performance overhead."]}),"\n",(0,t.jsxs)(n.p,{children:["User-level threads (Fiber) have emerged as a key technology to address this issue, thanks to their lightweight scheduling and low context-switching costs. However, mainstream Fiber-related models (N:1 coroutines, M",":N"," kthread, melon Fiber, etc.) exhibit significant differences in ",(0,t.jsx)(n.strong,{children:"memory contention management mechanisms, resource overhead, and maintenance costs"}),". This document provides precise selection guidance by analyzing the underlying implementation logic and technical attributes of each model from both CPU and memory contention perspectives."]}),"\n",(0,t.jsx)(n.h2,{id:"2-core-architecture-and-characteristics-of-each-model-supplemented-with-memory-contention-management",children:"2. Core Architecture and Characteristics of Each Model (Supplemented with Memory Contention Management)"}),"\n",(0,t.jsx)(n.p,{children:"The fundamental differences between these models stem from their underlying architectural designs, which directly determine CPU utilization efficiency, memory contention management costs, and development thresholds. Below is an analysis of each model's core architecture, with a focus on memory contention management details."}),"\n",(0,t.jsx)(n.h3,{id:"21-n1-coroutines-single-threaded",children:"2.1 N:1 Coroutines (Single-threaded)"}),"\n",(0,t.jsxs)(n.p,{children:["The core architecture of N:1 coroutines is ",(0,t.jsx)(n.strong,{children:"a single kernel thread hosting all user-level coroutines"}),", with context switching implemented via ",(0,t.jsx)(n.code,{children:"ucontext"})," or ",(0,t.jsx)(n.code,{children:"fcontext"}),", taking only 100-200 nanoseconds per switch."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CPU Contention Characteristics"}),": No multi-core CPU contention; all coroutines execute serially without requiring locking mechanisms."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Contention Management Costs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Shared Memory"}),": No multi-core memory contention; only serial read/write operations on local and global variables within coroutines need to be handled, resulting in ",(0,t.jsx)(n.strong,{children:"extremely low"})," management costs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stack Resources"}),": Each coroutine has an independent user-level stack with flexibly configurable size (usually several KB to tens of KB), offering much lower memory overhead than kernel thread stacks. However, there is a high risk of stack overflow, requiring careful pre-planning of stack sizes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Coherence"}),": No multi-core cache synchronization overhead in a single-threaded architecture, resulting in extremely high data cache hit rates and no cache thrashing issues."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limitations"}),": Blocking of a single coroutine will stall all coroutines in the thread. Full asynchronous refactoring is required to adapt to blocking scenarios, leading to a sharp increase in code complexity. Suitable only for ultra-simple, IO-bound scenarios with no multi-core requirements."]}),"\n",(0,t.jsxs)(n.h3,{id:"22-m-kthread",children:["2.2 M",":N"," kthread"]}),"\n",(0,t.jsxs)(n.p,{children:["The M",":N"," kthread adopts an architecture of ",(0,t.jsx)(n.strong,{children:"multiple user-level Fibers mapped to a small number of kernel pthread threads"}),", with pthread resource pools managed by ",(0,t.jsx)(n.code,{children:"TaskGroup"})," and a built-in ",(0,t.jsx)(n.code,{children:"work stealing"})," scheduler for cross-core load balancing."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CPU Contention Characteristics"}),": Multi-core CPU contention is balanced through ",(0,t.jsx)(n.code,{children:"work stealing"})," scheduling. Blocked Fibers voluntarily yield their pthread, ensuring high CPU utilization."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Contention Management Costs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Shared Memory"}),": Shared resources across ",(0,t.jsx)(n.code,{children:"TaskGroup"})," instances require synchronization via ",(0,t.jsx)(n.code,{children:"butex"})," (a kthread-specific synchronization primitive). ",(0,t.jsx)(n.code,{children:"butex"})," combines the advantages of spinlocks and condition variables\u2014with minimal spin overhead under low contention and automatic hibernation under high contention\u2014resulting in ",(0,t.jsx)(n.strong,{children:"moderate"})," memory contention management costs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stack Resources"}),": All Fibers reuse the user-level stack pool of ",(0,t.jsx)(n.code,{children:"TaskGroup"}),", minimizing stack creation/destruction overhead. Dynamic stack expansion is supported for high memory utilization. Attention should be paid to stack pool size configuration to avoid performance degradation caused by pool exhaustion."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Coherence"}),": The ",(0,t.jsx)(n.code,{children:"work stealing"})," scheduler attempts to execute Fibers on the same CPU core (affinity optimization), reducing multi-core cache synchronization overhead. Batch task processing further improves cache locality and mitigates cache thrashing caused by memory contention."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limitations"}),": Requires understanding of underlying concepts such as ",(0,t.jsx)(n.code,{children:"TaskGroup"}),", stack pools, and ",(0,t.jsx)(n.code,{children:"butex"}),", leading to a high development threshold. Must be adapted to the krpc ecosystem. Suitable for high-concurrency, high-contention scenarios."]}),"\n",(0,t.jsx)(n.h3,{id:"23-melon-fiber",children:"2.3 melon Fiber"}),"\n",(0,t.jsxs)(n.p,{children:["The core architecture of melon Fiber is ",(0,t.jsx)(n.strong,{children:"1:1 mapping + user-level stack reuse"}),"\u2014each Fiber corresponds to an independent kernel pthread thread, with the key optimization being user-level stack reuse to reduce creation/destruction overhead."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CPU Contention Characteristics"}),": Multi-core CPU contention relies on kernel scheduling with no Fiber-level load balancing. Blocked Fibers occupy kernel threads."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Contention Management Costs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Shared Memory"}),": Relies entirely on pthread native locks (",(0,t.jsx)(n.code,{children:"mutex"}),"/",(0,t.jsx)(n.code,{children:"condition_variable"}),") for synchronization. Frequent lock contention occurs in high-contention scenarios, resulting in ",(0,t.jsx)(n.strong,{children:"high"})," memory contention management costs. No special optimization mechanisms are provided; lock granularity must be manually designed to avoid deadlocks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stack Resources"}),": Stack reuse reduces memory allocation overhead, but each Fiber still corresponds to a kernel-thread-level stack (usually several MB), with much higher memory overhead than the user-level stacks of M",":N"," kthread. Stack management is simple, requiring no attention to underlying details, thus lowering development costs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Coherence"}),": No affinity optimization; Fiber scheduling depends on the kernel, leading to frequent cross-core execution, cache invalidation, and thrashing. In high-concurrency scenarios, cache issues caused by memory contention significantly amplify performance losses."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Advantages"}),": Extremely simple API, allowing direct migration of synchronous code with low development and maintenance costs. Suitable for low-contention, concurrency-controlled scenarios."]}),"\n",(0,t.jsx)(n.h3,{id:"24-executionqueue",children:"2.4 ExecutionQueue"}),"\n",(0,t.jsxs)(n.p,{children:["ExecutionQueue is not a general-purpose Fiber model; its core architecture is a ",(0,t.jsx)(n.strong,{children:"wait-free asynchronous serial queue"}),", where a single pthread processes all tasks in the queue serially."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CPU Contention Characteristics"}),": No CPU contention; serial execution results in zero multi-core utilization."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Contention Management Costs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Shared Memory"}),": Eliminates memory contention entirely. All tasks read/write shared resources serially without requiring any locking mechanisms, achieving ",(0,t.jsx)(n.strong,{children:"ultra-low"})," management costs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stack Resources"}),": Reuses the stack of the executing thread with no additional stack overhead. Task data is transmitted via the queue; attention should be paid to queue memory usage to avoid memory overflow."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Coherence"}),": Serial task execution ensures continuous access to shared resources, achieving a near-100% cache hit rate with no cache thrashing. Batch processing further reduces memory access latency."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Limitations"}),": No multi-core parallel processing capability. Suitable only as a supplement to other models for solving sub-problems requiring high-contention, ordered execution (e.g., shared resource read/write, batch log persistence)."]}),"\n",(0,t.jsx)(n.h3,{id:"25-mutexpthread",children:"2.5 Mutex+pthread"}),"\n",(0,t.jsx)(n.p,{children:"Mutex+pthread is the most basic concurrency model, relying on pure kernel threads and native locking mechanisms with no user-level scheduling capabilities."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CPU Contention Characteristics"}),": Multi-core CPU contention relies on kernel scheduling. Blocked threads occupy kernel resources, resulting in low CPU utilization."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Contention Management Costs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Shared Memory"}),": Relies on coarse-grained locking for synchronization. Severe lock contention occurs in high-contention scenarios, with high risks of deadlocks and priority inversion, leading to ",(0,t.jsx)(n.strong,{children:"extremely high"})," memory contention management costs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stack Resources"}),": Each thread corresponds to a kernel stack of several MB, resulting in high memory overhead. Stack management is handled by the kernel, requiring no developer intervention."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Coherence"}),": The randomness of kernel scheduling leads to frequent cross-core execution, severe cache invalidation, prominent cache thrashing, and poor memory access performance."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Applicable Scenarios"}),": Low-concurrency, simple logic utility programs that do not require performance optimization."]}),"\n",(0,t.jsx)(n.h2,{id:"3-comprehensive-comparison-of-all-models-added-memory-contention-management-cost-dimension",children:"3. Comprehensive Comparison of All Models (Added Memory Contention Management Cost Dimension)"}),"\n",(0,t.jsxs)(n.p,{children:["The following table provides a horizontal comparison of the models across five core dimensions: ",(0,t.jsx)(n.strong,{children:"multi-core CPU utilization efficiency"}),", ",(0,t.jsx)(n.strong,{children:"memory contention management cost"}),", ",(0,t.jsx)(n.strong,{children:"ease of use"}),", ",(0,t.jsx)(n.strong,{children:"synchronous code migration cost"}),", and ",(0,t.jsx)(n.strong,{children:"typical applicable technical scenarios"}),". Memory contention management costs are classified into five levels (Extremely Low/Low/Moderate/High/Extremely High), while ease of use is rated on a 5-star scale (\u2605)."]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Multi-core CPU Utilization Efficiency"}),(0,t.jsx)(n.th,{children:"Memory Contention Management Cost"}),(0,t.jsx)(n.th,{children:"Ease of Use (\u2605)"}),(0,t.jsx)(n.th,{children:"Synchronous Code Migration Cost"}),(0,t.jsx)(n.th,{children:"Typical Applicable Technical Scenarios"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"N:1 Coroutines"}),(0,t.jsx)(n.td,{children:"None (single-threaded)"}),(0,t.jsx)(n.td,{children:"Extremely Low"}),(0,t.jsx)(n.td,{children:"\u2605\u2605\u2605\u2605"}),(0,t.jsx)(n.td,{children:"Extremely High"}),(0,t.jsx)(n.td,{children:"Ultra-simple IO-bound services with no multi-core requirements (e.g., small HTTP proxies, lightweight crawlers)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsxs)(n.td,{children:["M",":N"," kthread"]}),(0,t.jsx)(n.td,{children:"High (work stealing scheduling)"}),(0,t.jsx)(n.td,{children:"Moderate"}),(0,t.jsx)(n.td,{children:"\u2605"}),(0,t.jsx)(n.td,{children:"Moderate"}),(0,t.jsx)(n.td,{children:"High-concurrency, high-contention scenarios (e.g., large-scale search services, real-time data processing platforms)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"melon Fiber"}),(0,t.jsx)(n.td,{children:"Moderate (kernel-dependent scheduling)"}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"\u2605\u2605\u2605\u2605\u2605"}),(0,t.jsx)(n.td,{children:"Extremely Low"}),(0,t.jsx)(n.td,{children:"Low-contention, concurrency-controlled scenarios (e.g., approval process backends, data synchronization services)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ExecutionQueue"}),(0,t.jsx)(n.td,{children:"None (serial execution)"}),(0,t.jsx)(n.td,{children:"Extremely Low"}),(0,t.jsx)(n.td,{children:"\u2605\u2605"}),(0,t.jsx)(n.td,{children:"Moderate"}),(0,t.jsx)(n.td,{children:"Sub-problems requiring high-contention, ordered execution (e.g., shared resource read/write, batch log persistence)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Mutex+pthread"}),(0,t.jsx)(n.td,{children:"Moderate (kernel-dependent scheduling)"}),(0,t.jsx)(n.td,{children:"Extremely High"}),(0,t.jsx)(n.td,{children:"\u2605\u2605\u2605"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Low-concurrency, simple logic utility programs (e.g., log collection scripts, configuration verification tools)"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"4-precise-selection-recommendations-incorporating-memory-contention-management-costs",children:"4. Precise Selection Recommendations (Incorporating Memory Contention Management Costs)"}),"\n",(0,t.jsxs)(n.p,{children:["The core of model selection lies in balancing ",(0,t.jsx)(n.strong,{children:"CPU contention requirements"}),", ",(0,t.jsx)(n.strong,{children:"memory contention intensity"}),", and ",(0,t.jsx)(n.strong,{children:"development/maintenance costs"}),", with the following specific criteria:"]}),"\n",(0,t.jsxs)(n.h3,{id:"41-priority-selection-m-kthread",children:["4.1 Priority Selection: M",":N"," kthread"]}),"\n",(0,t.jsxs)(n.p,{children:["Choose M",":N"," kthread when the business meets the following criteria:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["High concurrency and high contention (",(0,t.jsx)(n.code,{children:"QPS \xd7 latency"})," far exceeds the number of CPU cores);"]}),"\n",(0,t.jsx)(n.li,{children:"Frequent read/write operations on shared memory (e.g., global caches, counters);"}),"\n",(0,t.jsx)(n.li,{children:"Performance priority outweighs development costs."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Rationale"}),": Moderate memory contention management costs. ",(0,t.jsx)(n.code,{children:"butex"})," and ",(0,t.jsx)(n.code,{children:"work stealing"})," balance synchronization efficiency and cache coherence, maximizing multi-core resource utilization. Ideal for core businesses with stringent performance requirements."]}),"\n",(0,t.jsx)(n.h3,{id:"42-priority-selection-melon-fiber",children:"4.2 Priority Selection: melon Fiber"}),"\n",(0,t.jsx)(n.p,{children:"Choose melon Fiber when the business meets the following criteria:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Low contention (low-frequency read/write operations on shared resources, with critical section execution time < 1\u03bcs);"}),"\n",(0,t.jsxs)(n.li,{children:["Controlled concurrency (",(0,t.jsx)(n.code,{children:"QPS \xd7 latency"})," \u2264 2 \xd7 number of CPU cores);"]}),"\n",(0,t.jsx)(n.li,{children:"Development/maintenance cost priority outweighs performance optimization."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Rationale"}),": High memory contention management costs, but extremely low development threshold. Synchronous code can be directly migrated, making it suitable for rapid deployment scenarios where no investment is required for memory contention optimization."]}),"\n",(0,t.jsx)(n.h3,{id:"43-priority-selection-executionqueue",children:"4.3 Priority Selection: ExecutionQueue"}),"\n",(0,t.jsxs)(n.p,{children:["Introduce ExecutionQueue as a supplement regardless of the main process model when the business has ",(0,t.jsx)(n.strong,{children:"sub-modules requiring high-contention, ordered execution"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Rationale"}),": Ultra-low memory contention management costs, completely eliminating lock contention and cache thrashing. It is the optimal solution for high-contention memory scenarios."]}),"\n",(0,t.jsx)(n.h3,{id:"44-priority-selection-n1-coroutines",children:"4.4 Priority Selection: N:1 Coroutines"}),"\n",(0,t.jsxs)(n.p,{children:["Consider N:1 coroutines ",(0,t.jsx)(n.strong,{children:"only"})," when the business is ",(0,t.jsx)(n.strong,{children:"purely IO-bound with no multi-core requirements"}),", and the team has mature asynchronous programming capabilities."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Rationale"}),": Extremely low memory contention management costs, but requires full asynchronous refactoring, making it suitable for extremely limited scenarios."]}),"\n",(0,t.jsx)(n.h2,{id:"5-conclusion",children:"5. Conclusion"}),"\n",(0,t.jsxs)(n.p,{children:["Fiber model selection essentially involves balancing ",(0,t.jsx)(n.strong,{children:"CPU utilization"}),", ",(0,t.jsx)(n.strong,{children:"memory contention management"}),", and ",(0,t.jsx)(n.strong,{children:"development costs"}),". Memory contention management cost is an easily overlooked yet critical dimension:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For high-contention memory scenarios, prioritize the ",(0,t.jsxs)(n.strong,{children:["M",":N"," kthread + ExecutionQueue"]})," combination to balance multi-core parallelism and memory contention optimization;"]}),"\n",(0,t.jsxs)(n.li,{children:["For low-contention, rapid deployment scenarios, prioritize ",(0,t.jsx)(n.strong,{children:"melon Fiber"}),", trading development efficiency for performance costs;"]}),"\n",(0,t.jsxs)(n.li,{children:["For extremely simple scenarios, ",(0,t.jsx)(n.strong,{children:"N:1 coroutines"})," or ",(0,t.jsx)(n.strong,{children:"Mutex+pthread"})," can be selected without over-engineering."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In practical development, it is recommended to verify selection rationality through ",(0,t.jsx)(n.strong,{children:"memory contention analysis tools (e.g., cachegrind) + stress testing"}),", focusing on matching technical attributes with business requirements."]}),"\n",(0,t.jsx)(n.h2,{id:"6-final-summary",children:"6. Final Summary"}),"\n",(0,t.jsxs)(n.p,{children:["The ultimate value of Fiber technology is to resolve the contradiction between asynchronous execution and sequential code writing\u2014it allows developers to implement asynchronous task scheduling using synchronous code logic, avoiding code fragmentation caused by callback hell and reducing the mental burden of multi-threaded programming. However, in C++ development scenarios, ",(0,t.jsx)(n.strong,{children:"memory lifecycle management is always the most critical aspect of any concurrent model"}),". Issues such as dangling pointers, invalid references, contention conflicts, and memory leaks directly lead to program crashes or performance degradation, and their importance far outweighs that of scheduling models themselves. Regardless of the Fiber model chosen, two core issues must be clarified first: first, ",(0,t.jsx)(n.strong,{children:"memory contention state management"}),"\u2014identifying shared vs. exclusive memory and selecting appropriate synchronization primitives for shared memory (e.g., kthread's butex, ExecutionQueue's serialization, melon Fiber's pthread locks) to avoid unnecessary lock contention or cache thrashing; second, ",(0,t.jsx)(n.strong,{children:"memory lifecycle management"}),"\u2014defining the creator, holder, and releaser of memory resources, and planning memory release timing based on Fiber stack characteristics (e.g., M",":N"," kthread's stack pool reuse, melon Fiber's user-level stack reuse) to prevent memory leaks or dangling references caused by Fiber scheduling switches. In summary, Fiber model selection is not a simple choice of technical framework, but a comprehensive decision based on ",(0,t.jsx)(n.strong,{children:"memory management strategy + concurrent scheduling strategy"})," tailored to specific business scenarios. Only by clarifying memory-related issues can the performance and development efficiency advantages of Fiber technology be truly realized."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>c});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);